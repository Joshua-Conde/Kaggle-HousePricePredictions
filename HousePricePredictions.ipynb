{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cda0eb-3f31-46f9-ac1e-610d111845a2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf854758-5a4a-4618-8472-ac7ea489f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycaret\n",
    "# !pip install catboost xgboost optuna\n",
    "\n",
    "# !pip install --force-reinstall threadpoolctl \n",
    "# ^ This WORKED in fixing the \"knn_impute()\" - related error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e849-f51f-4d1d-b10e-784ddd4a77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra\n",
    "import pandas as pd # DataFrame manipulation\n",
    "pd.options.display.max_columns = 500 \n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid') # This makes our plots pretty, but it's completely unnecessary \n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor # KNeighborsRegressor is used for numeric imputation purposes\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import StandardScaler # StandardScaler is used for numeric scaling purposes\n",
    "from pycaret.regression import setup, compare_models # Pycaret is a lovely \"low/no-code\" tool to aid us in our model selection(s)\n",
    "from sklearn.model_selection import KFold, cross_val_score # cross_val_score measures a model's performance\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge, HuberRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna # Optuna is a quicker alternative to GridSearchCV; an impatient person's dream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b9ec6-dd96-4c4d-8d0b-0c1a76640646",
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = pd.read_csv('/Users/joshuaconde/GitHub-REPOSITORIES/Kaggle-HousePricePredictions/HousePricePredictions/train.csv')\n",
    "test0 = pd.read_csv('/Users/joshuaconde/GitHub-REPOSITORIES/Kaggle-HousePricePredictions/HousePricePredictions/test.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('/Users/joshuaconde/GitHub-REPOSITORIES/Kaggle-HousePricePredictions/HousePricePredictions/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f94a60-0d6a-4d35-8ce3-c503d11c5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b20c6b-e7fb-45d2-a003-438a6817b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I understand test0's not having the target variable as its final column, but why is there (in addition to this) one less row?\n",
    "\n",
    "# test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54292cbb-0e8b-4a3a-a858-c6f5cb8bd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to SalePrice's continuous nature, we need to derive (from memory) some kind of a regression model\n",
    "\n",
    "# sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ba0f5-3bfb-490f-9852-83f381581e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# DATA PREPROCESSING (CLEANING) PIPELINE:\n",
    "###\n",
    "# NUMERIC IMPUTATION (By means of \".mean()\" and/or the \"KNN\" strategy) \n",
    "# CATEGORICAL IMPUTATION (Ordinal(s) = \"N;\" Regular Categorical(s) = \".mode()\")\n",
    "\n",
    "# NUMERIC SCALING (\"StandardScaler()\")\n",
    "# CATEGORICAL ENCODING (\"One hot encoding\")\n",
    "\n",
    "# FEATURE TRANSFORMATION (\".log1p()\")\n",
    "# TARGET TRANSFORMATION (\".log()\")\n",
    "# ^ Don't forget about the finishing \".exp()\"\n",
    "\n",
    "# FEATURE SELECTION - There's no need to perform this step unless the column count exceeds ~1k\n",
    "# FEATURE ENGINEERING - We'll perform this following our testing the performance of at least 1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7e5db-ab9f-4e2b-9462-71f961533ed1",
   "metadata": {},
   "source": [
    "## 1. DataFrame Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c4cc9-c0d7-4a0f-9c7b-23e5db2fab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This early on, in the Data Preprocessing stage, it's totally valid to combine the training AND testing sets\n",
    "# It's crucial, however, that we (one again) separate them come time for any model \".fit()\"ing whatsoever\n",
    "\n",
    "target = train0['SalePrice']\n",
    "test_ids = test0['Id'] # This is only needed for our soon-to-come submission\n",
    "\n",
    "train1 = train0.drop(['Id', 'SalePrice'], axis=1) \n",
    "# ^ We're to run a .drop() (on the training data) to effectively erase any unique Id's and target variable(s) at play\n",
    "test1 = test0.drop('Id', axis=1)\n",
    "\n",
    "# \"data,\" here, pertains to the now-COMBINED DataFrame\n",
    "data0 = pd.concat([train1, test1], axis=0).reset_index(drop=True)\n",
    "\n",
    "data0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf8a7-0000-43ec-9cc4-9753763252d4",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30287cf5-2b87-46d9-81b1-c5f2c75b46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll, for goals related to consistency, re-initialize a copy of our data set every time the \n",
    "# number in bold is incremented\n",
    "\n",
    "data1 = data0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b5b50-cd85-46b5-a9d5-0824341bfb0b",
   "metadata": {},
   "source": [
    "### 2A. Ensure Proper Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264af133-b71c-4d12-be01-8d6a00660853",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['MSSubClass'] = data1['MSSubClass'].astype(str) # This feature is totally nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72de79-2618-4100-a447-20e9a41b3e2d",
   "metadata": {},
   "source": [
    "### 2B. Fill Ordinal AND Regular Categorical Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d536f-931d-4397-a695-e2dde2f6ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal imputations can be done via. through our careful use of some unique, constant value\n",
    "for column in [\n",
    "    'Alley',\n",
    "    'BsmtQual',\n",
    "    'BsmtCond',\n",
    "    'BsmtExposure',\n",
    "    'BsmtFinType1',\n",
    "    'BsmtFinType2',\n",
    "    'FireplaceQu',\n",
    "    'GarageType',\n",
    "    'GarageFinish',\n",
    "    'GarageQual',\n",
    "    'GarageCond',\n",
    "    'PoolQC',\n",
    "    'Fence',\n",
    "    'MiscFeature'\n",
    "]: data1[column] = data1[column].fillna(\"N\")\n",
    "\n",
    "# Regular categoricals can be imputed with the popular \".mode()\"\n",
    "for column in [\n",
    "    'MSZoning',\n",
    "    'Utilities',\n",
    "    'Exterior1st',\n",
    "    'Exterior2nd',\n",
    "    'MasVnrType',\n",
    "    'Electrical',\n",
    "    'KitchenQual',\n",
    "    'Functional',\n",
    "    'SaleType'\n",
    "]: data1[column] = data1[column].fillna(data1[column].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b54467-95ea-4561-b3bd-9fc1af6eac16",
   "metadata": {},
   "source": [
    "### 2C. Fill Numeric Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae63d8-5384-4b58-9f17-38fcf08571a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute(df, na_target):\n",
    "    df = df.copy()\n",
    "    \n",
    "    numeric_df = df.select_dtypes(np.number) # Selects all numeric columns\n",
    "    non_na_columns = numeric_df.loc[ : , numeric_df.isna().sum() == 0].columns # Selects all numeric columns that have NO empty values\n",
    "    \n",
    "    X_train = numeric_df.loc[numeric_df[na_target].isna() == False, non_na_columns]\n",
    "    # ^ Selects the feature columns (non_na_columns) for rows where the target column does not have missing values\n",
    "    y_train = numeric_df.loc[numeric_df[na_target].isna() == False, na_target]\n",
    "    # ^ Selects the values of the target column (na_target) for rows where it does not have missing values\n",
    "\n",
    "    # \"X_train\" contains the features and \"y_train\" contains the corresponding target values for rows where the target column \n",
    "    # (na_target) does not have missing values\n",
    "    \n",
    "    X_test = numeric_df.loc[numeric_df[na_target].isna() == True, non_na_columns]\n",
    "    \n",
    "    knn = KNeighborsRegressor()\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    df.loc[df[na_target].isna() == True, na_target] = y_pred\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415177d-ffd1-4ecd-9bea-e099a4954194",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\n",
    "    'LotFrontage',\n",
    "    'MasVnrArea',\n",
    "    'BsmtFinSF1',\n",
    "    'BsmtFinSF2',\n",
    "    'BsmtUnfSF',\n",
    "    'TotalBsmtSF',\n",
    "    'BsmtFullBath',\n",
    "    'BsmtHalfBath',\n",
    "    'GarageYrBlt',\n",
    "    'GarageCars',\n",
    "    'GarageArea'\n",
    "]: data1 = knn_impute(data1, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cc1cb-aac0-4f8a-bc3e-fa1028577162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2d841-4c68-4cd5-9e74-fbd467267700",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2911ad-cb11-43e0-a3e1-e7124109ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.copy()\n",
    "\n",
    "data2[\"SqFtPerRoom\"] = ( data2[\"GrLivArea\"] / (data2[\"TotRmsAbvGrd\"] +\n",
    "                         data2[\"FullBath\"] +\n",
    "                         data2[\"HalfBath\"] +\n",
    "                         data2[\"KitchenAbvGr\"]) )\n",
    "\n",
    "data2['Total_Home_Quality'] = data1['OverallQual'] + data1['OverallCond']\n",
    "\n",
    "data2['Total_Bathrooms'] = (data2['FullBath'] + (0.5 * data2['HalfBath']) +\n",
    "                            data2['BsmtFullBath'] + (0.5 * data2['BsmtHalfBath']))\n",
    "\n",
    "data2[\"HighQualSF\"] = data2[\"1stFlrSF\"] + data2[\"2ndFlrSF\"]\n",
    "\n",
    "data2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d077031-41a0-406a-a343-2ae8b02d363c",
   "metadata": {},
   "source": [
    "## 4. Feature Transformations -> .log1p()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f4a2d-f7f2-4bc5-a7fe-b476243c71e4",
   "metadata": {},
   "source": [
    "### 4A. Log Transform(ation) for Skewed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bbda9-a35f-476b-bcd7-431bc0d979f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.copy()\n",
    "\n",
    "skewed_df = pd.DataFrame(data3.select_dtypes(np.number).columns, columns=['Feature'])\n",
    "skewed_df['Skew'] = skewed_df['Feature'].apply(lambda feature: scipy.stats.skew(data3[feature]))\n",
    "skewed_df['Absolute Skew'] = skewed_df['Skew'].apply(abs)\n",
    "skewed_df['Skewed'] = skewed_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False) # 0.5 is a pretty regular  \n",
    "                                                                                        # \"cut-off\" as far as this context goes\n",
    "\n",
    "skewed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82719cea-22ec-4b32-a12b-0d03843b0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in skewed_df.query(\"Skewed == True\")['Feature'].values:\n",
    "    data3[column] = np.log1p(data3[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24d5a4-048a-487f-aeed-dce2b77de753",
   "metadata": {},
   "source": [
    "### 4B. Cosine Transform(ation) for Cyclical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78a0a5-09c7-4c84-86d7-187b6562331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['MoSold'] = (-np.cos(0.5236 * data3['MoSold']))\n",
    "\n",
    "data2['MoSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc726f6b-9343-4b68-ac0f-1138df928d85",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdffa0-ebe2-4daf-806e-1f93bff20772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data3.copy()\n",
    "\n",
    "data4 = pd.get_dummies(data4)\n",
    "\n",
    "data4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee45a8-f112-4e19-bd72-4b84f4067cfb",
   "metadata": {},
   "source": [
    "## 6. Numeric Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4719dab-2417-4242-9bc0-a1dbab3b26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data4.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data5)\n",
    "\n",
    "data5 = pd.DataFrame(scaler.transform(data5), index=data5.index, columns=data5.columns)\n",
    "\n",
    "data5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159a668-8c9c-48e8-a5fe-c59bfe7fd381",
   "metadata": {},
   "source": [
    "## 7. Target Transformation -> .log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98caaaf4-507c-4464-a8d0-c599732fab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(target, kde=True, fit=scipy.stats.norm)\n",
    "plt.title(\"Without Log Transform\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)\n",
    "plt.xlabel(\"Log SalePrice\")\n",
    "plt.title(\"With Log Transform\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402cbe6-0038-4d5a-be37-658ccf7f9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_target = np.log(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99346ab-cebd-44aa-b418-27c08b44e277",
   "metadata": {},
   "source": [
    "## 8. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fe551-4233-4069-9f8f-2a59e17d866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like to consider it strictly ILLEGAL to ever \".fit()\" some model on a combined train(ing)/test(ing) set\n",
    "\n",
    "train_final = data5.loc[:train0.index.max(), :].copy()\n",
    "test_final = data5.loc[train0.index.max() + 1:, :].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be893fd-a256-4b28-92e1-da82001ffe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25f7c1-f32b-4164-ac64-535ae4a19e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e906d7-746b-4914-a0ce-8d6881b5e98b",
   "metadata": {},
   "source": [
    "# Pycaret's Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c0305-12dc-4f9c-8e6f-7d11131dda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = setup(data=pd.concat([train_final, log_target], axis=1), target='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f08806-2302-4fbc-a631-af3085adc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f02723-da9a-4083-a70b-b5cbbb8aa1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "RESULTS:\n",
    "1. catboost regressor (SAME)\n",
    "2. gradient boosting regressor\n",
    "3. light gradient boosting machine (SAME)\n",
    "4. bayesian ridge (SAME)\n",
    "5. extra trees regressor\n",
    "-\n",
    "6. extreme gradient boosting\n",
    "7. random forest regressor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317beb85-3e98-4226-8822-eb09fce89f93",
   "metadata": {},
   "source": [
    "## DELETED: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c045cf-b9da-4c04-8d2d-eff3972d7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model = CatBoostRegressor(verbose=0)\n",
    "\n",
    "# baseline_model.fit(train_final, log_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66531f1e-7fbb-4da1-a37a-479e5698cf78",
   "metadata": {},
   "source": [
    "## DELETED: Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2069c-8203-4a74-80ad-d500b8b19013",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) # -> 9/10 = Train(ing); 1/10 = Test(ing)\n",
    "\n",
    "# results = cross_val_score(baseline_model, train_final, log_target, scoring=\"neg_mean_squared_error\", cv=kf) # estimator, X, target\n",
    "\n",
    "# -results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015429c7-5fc9-4b44-bcf5-120579c6d2de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# np.exp(np.sqrt(np.mean(-results))) # this \".exp()\" is here, thanks, to the feature's prior leveraging of \".log(1p)()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a07b31-f210-42aa-8f07-c82b42827687",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d6902-3134-4c2b-9287-46e174990be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# log_target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f251a-baf1-4b87-bd97-4d8e12b065f6",
   "metadata": {},
   "source": [
    "## Bayesian Ridge Hyper-parameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f32d3-6d4a-4e77-ad77-59ebe94bd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def br_objective(trial):\n",
    "    n_iter = trial.suggest_int('n_iter', 50, 600)\n",
    "    tol = trial.suggest_loguniform('tol', 1e-8, 10.0)\n",
    "    alpha_1 = trial.suggest_loguniform('alpha_1', 1e-8, 10.0)\n",
    "    alpha_2 = trial.suggest_loguniform('alpha_2', 1e-8, 10.0)\n",
    "    lambda_1 = trial.suggest_loguniform('lambda_1', 1e-8, 10.0)\n",
    "    lambda_2 = trial.suggest_loguniform('lambda_2', 1e-8, 10.0)\n",
    "    alpha_init = trial.suggest_loguniform('alpha_init', 1e-8, 10.0)\n",
    "    lambda_init = trial.suggest_loguniform('lambda_init', 1e-8, 10.0)\n",
    "    compute_score = trial.suggest_categorical('compute_score', ['True', 'False'])\n",
    "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "    copy_X = trial.suggest_categorical('copy_X', [True, False])\n",
    "    verbose = trial.suggest_categorical('verbose', [True, False])\n",
    "    normalize = trial.suggest_categorical('normalize', [True, False])\n",
    "\n",
    "    model = BayesianRidge(\n",
    "        n_iter=n_iter,\n",
    "        tol=tol,\n",
    "        alpha_1=alpha_1,\n",
    "        alpha_2=alpha_2,\n",
    "        lambda_1=lambda_1,\n",
    "        lambda_2=lambda_2,\n",
    "        alpha_init=alpha_init,\n",
    "        lambda_init=lambda_init,\n",
    "        compute_score=compute_score,\n",
    "        fit_intercept=fit_intercept,\n",
    "        copy_X=copy_X,\n",
    "        verbose=verbose,\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "    model.fit(train_final, log_target)\n",
    "\n",
    "    cv_scores = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n",
    "    # this \".exp()\" is here, thanks, to the feature's prior leveraging of \".log(1p)()\"\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05d472-c9c2-4d20-8337-b2875f7d2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study = optuna.create_study(direction='minimize')\n",
    "#study.optimize(br_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1161e1a-0079-4893-9d2f-4fdc1ce78273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779cc99-181a-4f6b-a9b5-010eec6730d4",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor Hyper-parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07992582-69eb-4406-aaf2-8aab0fbfbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def gbr_objective(trial):\n",
    "    loss = trial.suggest_categorical('loss', ['squared_error', 'absolute_error', 'huber', 'quantile'])\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 600)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.1, 1.0)\n",
    "    criterion = trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error'])\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_impurity_decrease = trial.suggest_uniform('min_impurity_decrease', 0.0, 0.5)\n",
    "    init = trial.suggest_categorical('init', [None, 'zero'])\n",
    "    random_state = trial.suggest_categorical('random_state', [None, 42, 2022])\n",
    "    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    alpha = trial.suggest_uniform('alpha', 0.0, 0.99)\n",
    "    verbose = trial.suggest_categorical('verbose', [0, 1, 2])\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 10, 100)\n",
    "    warm_start = trial.suggest_categorical('warm_start', [True, False])\n",
    "    validation_fraction = trial.suggest_uniform('validation_fraction', 0.1, 0.5)\n",
    "    n_iter_no_change = trial.suggest_int('n_iter_no_change', 5, 20)\n",
    "    tol = trial.suggest_loguniform('tol', 1e-5, 1e-2)\n",
    "    ccp_alpha = trial.suggest_uniform('ccp_alpha', 0.0, 0.5)\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        subsample=subsample,\n",
    "        criterion=criterion,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "        max_depth=max_depth,\n",
    "        min_impurity_decrease=min_impurity_decrease,\n",
    "        init=init,\n",
    "        random_state=random_state,\n",
    "        max_features=max_features,\n",
    "        alpha=alpha,\n",
    "        verbose=verbose,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        warm_start=warm_start,\n",
    "        validation_fraction=validation_fraction,\n",
    "        n_iter_no_change=n_iter_no_change,\n",
    "        tol=tol,\n",
    "        ccp_alpha=ccp_alpha\n",
    "    )\n",
    "\n",
    "    model.fit(train_final, log_target)\n",
    "\n",
    "    cv_scores = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f28ea-abf2-472f-a5fa-3e0f16dac17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(gbr_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be886b-598d-4315-9f08-a635708c7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7c7ce-9e66-4662-8ab6-b3f34828ab0e",
   "metadata": {},
   "source": [
    "## Extra Trees Regressor Hyper-parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08798679-3876-4b3a-b7f1-0f9e8536f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def et_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 600)\n",
    "    criterion = trial.suggest_categorical('criterion', ['poisson', 'absolute_error', 'squared_error', 'friedman_mse'])\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 10, 100)\n",
    "    min_impurity_decrease = trial.suggest_uniform('min_impurity_decrease', 0.0, 0.5)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    oob_score = trial.suggest_categorical('oob_score', [True, False]) if bootstrap else False\n",
    "    n_jobs = trial.suggest_categorical('n_jobs', [-1, 1, 2, 4])\n",
    "    random_state = trial.suggest_categorical('random_state', [None, 42, 2022])\n",
    "    verbose = trial.suggest_categorical('verbose', [0, 1, 2])\n",
    "    warm_start = trial.suggest_categorical('warm_start', [True, False])\n",
    "    ccp_alpha = trial.suggest_uniform('ccp_alpha', 0.0, 0.5)\n",
    "    max_samples = trial.suggest_uniform('max_samples', 0.1, 1.0) if bootstrap else None\n",
    "    # monotonic_cst = trial.suggest_categorical('monotonic_cst', [None, 'increasing', 'decreasing'])\n",
    "\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "        max_features=max_features,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        min_impurity_decrease=min_impurity_decrease,\n",
    "        bootstrap=bootstrap,\n",
    "        oob_score=oob_score,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        verbose=verbose,\n",
    "        warm_start=warm_start,\n",
    "        ccp_alpha=ccp_alpha,\n",
    "        max_samples=max_samples,\n",
    "        # monotonic_cst=monotonic_cst\n",
    "    )\n",
    "\n",
    "    model.fit(train_final, log_target)\n",
    "\n",
    "    cv_scores = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067a31a-693e-409d-aecb-e2382b5f2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(et_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9531f3-8707-4501-90e6-67ea0fce9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba76a6-3c35-449a-9922-2e32683a2a0b",
   "metadata": {},
   "source": [
    "## Bagging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5c7e3-55b0-497b-b40e-65710c0b7d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Optuna allegedly defines for us the ideal(?) hyper-parameters, shown below\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 6000,\n",
    "    'learning_rate': 0.005,\n",
    "    'depth': 4,\n",
    "    'l2_leaf_reg': 1,\n",
    "    'eval_metric':'RMSE',\n",
    "    'early_stopping_rounds': 200,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "br_params = {\n",
    "    'n_iter': 304,\n",
    "    'tol': 0.16864712769300896,\n",
    "    'alpha_1': 5.589616542154059e-07,\n",
    "    'alpha_2': 9.799343618469923,\n",
    "    'lambda_1': 1.7735725582463822,\n",
    "    'lambda_2': 3.616928181181732e-06\n",
    "}\n",
    "\n",
    "lightgbm_params = {\n",
    "    'num_leaves': 39,\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 0.13705339989856127,\n",
    "    'n_estimators': 273,\n",
    "}\n",
    "\n",
    "gbr_params = {\n",
    "    'loss': 'absolute_error',\n",
    "    'learning_rate': 0.11054616145386358,\n",
    "    'n_estimators': 266,\n",
    "    'subsample': 0.5902941839375372,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'min_samples_split': 20,\n",
    "    'min_samples_leaf': 10,\n",
    "    'min_weight_fraction_leaf': 0.0175323040448155,\n",
    "    'max_depth': 10,\n",
    "    'min_impurity_decrease': 0.06678068552552889,\n",
    "    'init': 'zero',\n",
    "    'random_state': 2022,\n",
    "    'max_features': 'sqrt',\n",
    "    'alpha': 0.23585140032470903,\n",
    "    'verbose': 2,\n",
    "    'max_leaf_nodes': 15,\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.21488662066191244,\n",
    "    'n_iter_no_change': 8,\n",
    "    'tol': 0.0005401308587534491,\n",
    "    'ccp_alpha': 0.00151895861887127\n",
    "}\n",
    "\n",
    "et_params = {\n",
    "    'n_estimators': 531,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 20,\n",
    "    'min_weight_fraction_leaf': 0.09315416304656772,\n",
    "    'max_features': 'sqrt',\n",
    "    'max_leaf_nodes': 88,\n",
    "    'min_impurity_decrease': 0.41028883629550056,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 2022,\n",
    "    'verbose': 1,\n",
    "    'warm_start': True,\n",
    "    'ccp_alpha': 0.0023371871861925567,\n",
    "    'max_samples': 0.13295442632197021\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6aa61-7683-4fe0-91f3-0f8592d8495d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"catboost\": CatBoostRegressor(**catboost_params, verbose=0),\n",
    "    \"br\": BayesianRidge(**br_params),\n",
    "    \"lightgbm\": LGBMRegressor(**lightgbm_params),\n",
    "    \"gbr\": GradientBoostingRegressor(**gbr_params),\n",
    "    \"et\": ExtraTreesRegressor(**et_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af262c2d-1e59-4b68-8de0-c8ee6777e500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(train_final, log_target)\n",
    "    print(name + \" trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d66f4-265b-496f-9872-9049f1fa4f80",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266e487-7420-492b-a3a6-1062b018e5f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    result = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n",
    "    results[name] = result\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f470281-5cd8-4a2e-a3a1-da82477ee942",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "for name, result in results.items():\n",
    "    print(\"----------\\n\" + name)\n",
    "    print(np.mean(result))\n",
    "    print(np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79003b-171a-4062-8079-d1cdd35e5209",
   "metadata": {},
   "source": [
    "## Combine Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d006e-dcac-41ee-a461-472b1e649f0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "final_predictions = (\n",
    "    0.2 * np.exp(models['catboost'].predict(test_final)) +\n",
    "    0.2 * np.exp(models['br'].predict(test_final)) +\n",
    "    0.2 * np.exp(models['lightgbm'].predict(test_final)) +\n",
    "    0.2 * np.exp(models['gbr'].predict(test_final)) +\n",
    "    0.2 * np.exp(models['et'].predict(test_final))\n",
    ")\n",
    "\n",
    "final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379f80c-5586-405e-93b3-5a9443896587",
   "metadata": {},
   "source": [
    "## Prediction(s) Submission(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175aae30-027a-4d15-a028-2f464f1a5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test_ids, pd.Series(final_predictions, name='SalePrice')], axis=1)\n",
    "\n",
    "submission.to_csv('./submission.csv', index=False, header=True)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4024d2-f7b7-4187-8d01-963008066705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Score: 0.12312\n",
    "# 2. Score: 0.12813\n",
    "# 3. Score: 0.13826"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
